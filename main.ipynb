{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c21eb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from icecream import ic\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc7dae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = [10, 20, 50, 100, 200, 500, 1000]\n",
    "DENSITY = [0.2, 0.5, 0.8, 1.0]\n",
    "NOISE_LEVEL = [0.0, 0.1, 0.5, 0.8]\n",
    "NEGATIVE_VALUES = [False, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6737626",
   "metadata": {},
   "source": [
    "## Code to create the problem instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7aaf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_problem(\n",
    "    size: int,\n",
    "    *,\n",
    "    density: float = 1.0,\n",
    "    negative_values: bool = False,\n",
    "    noise_level: float = 0.0,\n",
    "    seed: int = 42,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Problem generator for Lab3\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    map = rng.random(size=(size, 2))\n",
    "    problem = rng.random((size, size))\n",
    "    if negative_values:\n",
    "        problem = problem * 2 - 1\n",
    "    problem *= noise_level\n",
    "    for a, b in product(range(size), repeat=2):\n",
    "        if rng.random() < density:\n",
    "            problem[a, b] += np.sqrt(\n",
    "                np.square(map[a, 0] - map[b, 0]) + np.square(map[a, 1] - map[b, 1])\n",
    "            )\n",
    "        else:\n",
    "            problem[a, b] = np.inf\n",
    "    np.fill_diagonal(problem, 0)\n",
    "    return (problem * 1_000).round(), map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861e687",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c10c502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_heuristic(u: int, v: int, pos_map: np.ndarray) -> float:\n",
    "    #pos_map[u] gives the (x, y) coordinates for node u\n",
    "    distance = np.linalg.norm(pos_map[u] - pos_map[v])\n",
    "    return distance\n",
    "\n",
    "def _reconstruct_path(came_from: Dict[int, int], current: int) -> List[int]:\n",
    "    \"\"\"Reconstructs the path from the 'came_from' dictionary.\"\"\"\n",
    "    path = [current]\n",
    "    while current in came_from:\n",
    "        current = came_from[current]\n",
    "        path.append(current)\n",
    "    return path[::-1] #Reverse to get path from start to goal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97824208",
   "metadata": {},
   "source": [
    "## A* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star_search(graph: nx.DiGraph, start_node: int, goal_node: int, pos_map: np.ndarray) -> Tuple[List[int], float, int]:\n",
    "    \"\"\"\n",
    "    Finds the shortest path between start_node and goal_node using A* search.\n",
    "\n",
    "    Args:\n",
    "        graph: The NetworkX DiGraph object (with 'weight' attributes).\n",
    "        start_node: The source node.\n",
    "        goal_node: The destination node.\n",
    "        pos_map: The (N, 2) array of node coordinates for the heuristic.\n",
    "\n",
    "    Returns:\n",
    "        A tuple: (list of nodes in the path, total path cost, nodes_explored)\n",
    "    \"\"\"\n",
    "\n",
    "    #g(n): Actual cost from start to n\n",
    "    g_score: Dict[int, float] = {node: float('inf') for node in graph.nodes}\n",
    "    g_score[start_node] = 0.0\n",
    "\n",
    "    #f(n) = g(n) + h(n): Estimated total cost from start through n to goal\n",
    "    f_score: Dict[int, float] = {node: float('inf') for node in graph.nodes}\n",
    "    #Initial f_score = 0 (g(start)) + h(start)\n",
    "    f_score[start_node] = euclidean_heuristic(start_node, goal_node, pos_map)\n",
    "\n",
    "    #Dictionary to store the cheapest path found so far to reach a node\n",
    "    came_from: Dict[int, int] = {}\n",
    "\n",
    "    #The set of discovered nodes that may need to be explored.\n",
    "    open_set: Dict[int, float] = {start_node: f_score[start_node]}\n",
    "\n",
    "    #Counter for explored nodes\n",
    "    nodes_explored = 0\n",
    "\n",
    "    while open_set:\n",
    "        #Get the node with the lowest f_score (Best-First Search component)\n",
    "        current_node = min(open_set, key=open_set.get)\n",
    "        nodes_explored += 1\n",
    "\n",
    "        if current_node == goal_node:\n",
    "            #Goal reached! Reconstruct and return the path.\n",
    "            return _reconstruct_path(came_from, current_node), g_score[goal_node], nodes_explored\n",
    "\n",
    "        #Remove current from the open set\n",
    "        del open_set[current_node]\n",
    "\n",
    "        #Explore neighbors\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            weight = graph.get_edge_data(current_node, neighbor)['weight']\n",
    "\n",
    "            #Tentative g_score is the cost to reach current + cost to reach neighbor\n",
    "            tentative_g_score = g_score[current_node] + weight\n",
    "\n",
    "            if tentative_g_score < g_score[neighbor]:\n",
    "                #This path to neighbor is better than any previous one.\n",
    "                came_from[neighbor] = current_node\n",
    "                g_score[neighbor] = tentative_g_score\n",
    "                f_score[neighbor] = tentative_g_score + euclidean_heuristic(neighbor, goal_node, pos_map)\n",
    "\n",
    "                if neighbor not in open_set:\n",
    "                    open_set[neighbor] = f_score[neighbor]\n",
    "\n",
    "    #If the loop finishes without reaching the goal\n",
    "    return [], float('inf'), nodes_explored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5444c918",
   "metadata": {},
   "source": [
    "## Reduced Cost A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf65df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_ford_potential(graph: nx.DiGraph, nodes: List[int]) -> Tuple[np.ndarray, bool]:\n",
    "    \"\"\"\n",
    "    With this version we still have a dummy source but it is connected to everyone.\n",
    "    (done by initializing all nodes to 0.0)\n",
    "    \"\"\"\n",
    "    num_nodes = len(nodes)\n",
    "    \n",
    "    # FIX: initialize everyone to 0.0 instead of infinity.\n",
    "    # by ensuring that disconnected nodes are active immediately, we catch all negative cycles.\n",
    "    potentials = {node: 0.0 for node in nodes}\n",
    "\n",
    "    #Relax edges to check for negative cycles.\n",
    "    for i in range(num_nodes): \n",
    "        is_last_pass = (i == num_nodes - 1)\n",
    "        changes_made = False\n",
    "        \n",
    "        for u, v, data in graph.edges(data=True):\n",
    "            weight = data['weight']\n",
    "            \n",
    "            # relaxation Logic\n",
    "            if potentials[u] + weight < potentials[v]:\n",
    "                if is_last_pass:\n",
    "                    # if we update on the Nth pass, there is a negative cycle\n",
    "                    return np.array([]), True\n",
    "                \n",
    "                potentials[v] = potentials[u] + weight\n",
    "                changes_made = True\n",
    "            \n",
    "            # optimization: If nothing changed this pass, we can stop early\n",
    "            if not changes_made:\n",
    "                break\n",
    "    \n",
    "    #No negative cycle detected\n",
    "    potential_array = np.array([potentials[node] for node in nodes])\n",
    "    return potential_array, False\n",
    "\n",
    "def reduced_cost_a_star_search(\n",
    "    graph: nx.DiGraph,\n",
    "    start_node: int,\n",
    "    goal_node: int,\n",
    "    pos_map: np.ndarray,\n",
    "    p_map: np.ndarray #The potential function array (p(v))\n",
    ") -> Tuple[List[int], float, int]:\n",
    "    \"\"\"Finds the shortest path using A* on reduced costs.\"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes)\n",
    "    node_to_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    g_score_prime: Dict[int, float] = {node: float('inf') for node in graph.nodes}\n",
    "    g_score_prime[start_node] = 0.0\n",
    "    came_from: Dict[int, int] = {}\n",
    "    nodes_explored = 0\n",
    "\n",
    "    #Heap stores tuples: (f_score_prime, node)\n",
    "    open_set_heap = []\n",
    "    \n",
    "    #f'(n) = g'(n) + h'(n) where h'(n) = h(n) + p(n)\n",
    "    h_prime_start = euclidean_heuristic(start_node, goal_node, pos_map) + p_map[node_to_index[start_node]]\n",
    "    initial_f_score = g_score_prime[start_node] + h_prime_start\n",
    "\n",
    "    heapq.heappush(open_set_heap, (initial_f_score, start_node))\n",
    "\n",
    "    DEBUG_LIMIT = 500000 \n",
    "    pop_count = 0\n",
    "\n",
    "    while open_set_heap:\n",
    "        pop_count += 1\n",
    "\n",
    "        if pop_count > DEBUG_LIMIT:\n",
    "            print(f\"!!! DEBUG BREAK: Exceeded pop limit ({DEBUG_LIMIT}) for path {start_node} -> {goal_node}. Abandoning search.\")\n",
    "            break\n",
    "\n",
    "        #Get the node with the lowest f_score (O(log V) operation)\n",
    "        f_score_current, current_node = heapq.heappop(open_set_heap)\n",
    "\n",
    "        #Guard against stale entries (nodes pushed multiple times with higher costs)\n",
    "        if f_score_current > g_score_prime[current_node] + euclidean_heuristic(current_node, goal_node, pos_map) + p_map[node_to_index[current_node]]:\n",
    "            continue #Skip this stale entry\n",
    "\n",
    "        nodes_explored += 1\n",
    "        \n",
    "        if current_node == goal_node:\n",
    "            path = _reconstruct_path(came_from, current_node)\n",
    "            #The true cost D(s, d) is recovered: D'(s, d) - p(s) + p(d)\n",
    "            start_index = node_to_index[start_node]\n",
    "            goal_index = node_to_index[goal_node]\n",
    "            true_cost = g_score_prime[goal_node] - p_map[start_index] + p_map[goal_index]\n",
    "            return path, true_cost, nodes_explored\n",
    "\n",
    "\n",
    "        for neighbor in graph.neighbors(current_node):\n",
    "            original_cost = graph.get_edge_data(current_node, neighbor)['weight']\n",
    "            \n",
    "            #Calculate the Reduced Cost of the edge\n",
    "            u_idx = node_to_index[current_node]\n",
    "            v_idx = node_to_index[neighbor]\n",
    "            reduced_cost = original_cost + p_map[u_idx] - p_map[v_idx]\n",
    "            \n",
    "            tentative_g_score_prime = g_score_prime[current_node] + reduced_cost\n",
    "            \n",
    "            if tentative_g_score_prime < g_score_prime[neighbor]:\n",
    "                came_from[neighbor] = current_node\n",
    "                g_score_prime[neighbor] = tentative_g_score_prime\n",
    "                \n",
    "                #Calculate f' score\n",
    "                h_prime_n = euclidean_heuristic(neighbor, goal_node, pos_map) + p_map[v_idx]\n",
    "                f_score_prime = tentative_g_score_prime + h_prime_n\n",
    "\n",
    "                #Push the neighbor onto the heap (O(log V) operation)\n",
    "                heapq.heappush(open_set_heap, (f_score_prime, neighbor))\n",
    "\n",
    "    return [], float('inf'), nodes_explored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adac204",
   "metadata": {},
   "source": [
    "## Approximation for large problem size\n",
    "used for the last 55 runs (approximately starting with size 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e3e1c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hub_based_apsp_approximation(\n",
    "    G: nx.DiGraph,\n",
    "    nodes: List[int],\n",
    "    pos_map: np.ndarray,\n",
    "    p_map: np.ndarray,\n",
    "    k: int = 5  #Number of hubs to select\n",
    ") -> Tuple[Dict[str, Any], int]:\n",
    "    \"\"\"\n",
    "    Approximates All-Pairs Shortest Path using k hubs.\n",
    "    \n",
    "    Returns: (apsp_paths_approximation, total_nodes_explored)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Hub Selection (Random Sampling)\n",
    "    #Ensure k is not larger than the total number of nodes\n",
    "    num_hubs = min(k, len(nodes))\n",
    "    hubs = random.sample(nodes, num_hubs)\n",
    "    \n",
    "    #Stores the distances D(u, h) - Shortest path from u to hub h\n",
    "    dist_to_hubs = {h: {} for h in hubs}\n",
    "    #Stores the distances D(h, v) - Shortest path from hub h to v\n",
    "    dist_from_hubs = {h: {} for h in hubs}\n",
    "    \n",
    "    total_nodes_explored = 0\n",
    "    \n",
    "    #SSSP calculations (2k runs of A* total)\n",
    "    \n",
    "    #Calculate D(h, v) for all h in hubs (k runs)\n",
    "    for h in hubs:\n",
    "        #Run A* from hub h to all other nodes v\n",
    "        for v in nodes:\n",
    "            if h == v:\n",
    "                dist_from_hubs[h][v] = 0.0\n",
    "                continue\n",
    "                \n",
    "            #Reuse single-source A* function\n",
    "            _, cost_hv, nodes_explored_hv = reduced_cost_a_star_search(G, h, v, pos_map, p_map)\n",
    "            dist_from_hubs[h][v] = cost_hv\n",
    "            total_nodes_explored += nodes_explored_hv\n",
    "\n",
    "    #Calculate D(u, h) for all h in hubs (k runs)\n",
    "    for h in hubs:\n",
    "        #Run A* from all other nodes u to hub h\n",
    "        for u in nodes:\n",
    "            if h == u:\n",
    "                dist_to_hubs[h][u] = 0.0\n",
    "                continue\n",
    "            \n",
    "            _, cost_uh, nodes_explored_uh = reduced_cost_a_star_search(G, u, h, pos_map, p_map)\n",
    "            dist_to_hubs[h][u] = cost_uh\n",
    "            total_nodes_explored += nodes_explored_uh\n",
    "\n",
    "    #Final approximation for all pairs\n",
    "    apsp_paths = {}\n",
    "    \n",
    "    for s, d in combinations(nodes, 2):\n",
    "        #Calculate min_h (D(s, h) + D(h, d))\n",
    "        min_approx_cost_sd = float('inf')\n",
    "        min_approx_cost_ds = float('inf')\n",
    "        \n",
    "        for h in hubs:\n",
    "            #Approximation for s -> d\n",
    "            D_sh = dist_to_hubs[h].get(s, float('inf'))\n",
    "            D_hd = dist_from_hubs[h].get(d, float('inf'))\n",
    "            min_approx_cost_sd = min(min_approx_cost_sd, D_sh + D_hd)\n",
    "            \n",
    "            #Approximation for d -> s\n",
    "            D_dh = dist_to_hubs[h].get(d, float('inf'))\n",
    "            D_hs = dist_from_hubs[h].get(s, float('inf'))\n",
    "            min_approx_cost_ds = min(min_approx_cost_ds, D_dh + D_hs)\n",
    "\n",
    "        #Store results (path is unavailable in this method, only cost)\n",
    "        apsp_paths[f\"{s}->{d}\"] = {\"path\": None, \"cost\": min_approx_cost_sd, \"cycle\": False}\n",
    "        apsp_paths[f\"{d}->{s}\"] = {\"path\": None, \"cost\": min_approx_cost_ds, \"cycle\": False}\n",
    "        \n",
    "    return apsp_paths, total_nodes_explored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a4b4c",
   "metadata": {},
   "source": [
    "## Run for all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee4c3577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_and_save(\n",
    "    problem_matrix: np.ndarray, \n",
    "    pos_map: np.ndarray, \n",
    "    instance_params: Dict[str, Any],\n",
    "    output_filename: str,\n",
    "    approximation_threshold: int = 200, #Use approximation for N > 200\n",
    "    num_hubs: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs A* APSP, collects all metrics, and saves the results to a file.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setup Graph\n",
    "    masked = np.ma.masked_array(problem_matrix, mask=np.isinf(problem_matrix))\n",
    "    G = nx.from_numpy_array(masked, create_using=nx.DiGraph)\n",
    "    nodes = list(G.nodes())\n",
    "    N = len(nodes)\n",
    "\n",
    "    #Calculate potentials (p_map) and check for negative cycles\n",
    "    p_map, cycle_detected = bellman_ford_potential(G, nodes)\n",
    "    \n",
    "    #Run A* and Time Measurement\n",
    "    \n",
    "    apsp_paths = {}\n",
    "    total_nodes_explored = 0\n",
    "    total_time = 0\n",
    "\n",
    "    #Determine whether to use exact A* or the approximation\n",
    "    is_approximation = N > approximation_threshold\n",
    "    \n",
    "    if cycle_detected:\n",
    "        #NEGATIVE CYCLE DETECTED\n",
    "        \n",
    "        #Save placeholder results for all pairs in this instance\n",
    "        for s, d in combinations(nodes, 2):\n",
    "            #Path s -> d and d -> s\n",
    "            apsp_paths[f\"{s}->{d}\"] = {\"path\": None, \"cost\": -float('inf'), \"cycle\": True}\n",
    "            apsp_paths[f\"{d}->{s}\"] = {\"path\": None, \"cost\": -float('inf'), \"cycle\": True}\n",
    "        \n",
    "    elif is_approximation:\n",
    "        #APPROXIMATION MODE (for large N)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        apsp_paths, total_nodes_explored = hub_based_apsp_approximation(\n",
    "            G, nodes, pos_map, p_map, k=num_hubs\n",
    "        )\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "    else:\n",
    "        #NO CYCLE: Run Reduced Cost A*\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for s, d in combinations(nodes, 2):\n",
    "            #Path s -> d\n",
    "            path_sd, cost_sd, nodes_sd = reduced_cost_a_star_search(G, s, d, pos_map, p_map)\n",
    "            apsp_paths[f\"{s}->{d}\"] = {\"path\": path_sd, \"cost\": cost_sd, \"cycle\": False}\n",
    "            total_nodes_explored += nodes_sd\n",
    "            \n",
    "            #Path d -> s\n",
    "            path_ds, cost_ds, nodes_ds = reduced_cost_a_star_search(G, d, s, pos_map, p_map)\n",
    "            apsp_paths[f\"{d}->{s}\"] = {\"path\": path_ds, \"cost\": cost_ds, \"cycle\": False}\n",
    "            total_nodes_explored += nodes_ds\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "    #Data Structuring and Saving\n",
    "    \n",
    "    experiment_data = {\n",
    "        \"parameters\": instance_params,\n",
    "        \"metrics\": {\n",
    "            \"total_time_seconds\": total_time,\n",
    "            \"total_nodes_explored\": total_nodes_explored,\n",
    "            \"number_of_pairs\": N * (N - 1),\n",
    "            \"negative_cycle_detected\": cycle_detected\n",
    "        },\n",
    "        #Store the matrix data so to can recreate the graph later\n",
    "        \"problem_matrix\": problem_matrix.tolist(), \n",
    "        \"node_coordinates\": pos_map.tolist(), \n",
    "        \"a_star_paths\": apsp_paths,\n",
    "    }\n",
    "\n",
    "    #Append to a file containing multiple experimental runs\n",
    "    #It's safest to append to a line-delimited JSON file (JSONL) for batch experiments\n",
    "    with open(output_filename, 'a') as f:\n",
    "        json.dump(experiment_data, f, default=lambda x: str(x) if isinstance(x, float) and abs(x) == float('inf') else x)\n",
    "        f.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980175cc",
   "metadata": {},
   "source": [
    "#### More utility functions\n",
    "The first part of the code computes all the combinations for the parameters\n",
    "\n",
    "The second part is a function to recover already completed runs and restart the algorithm from new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604378da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of experiment runs generated: 224\n",
      "Example of the first 3 runs:\n",
      "{'size': 10, 'density': 0.2, 'noise_level': 0.0, 'negative_values': False}\n",
      "{'size': 10, 'density': 0.2, 'noise_level': 0.0, 'negative_values': True}\n",
      "{'size': 10, 'density': 0.2, 'noise_level': 0.1, 'negative_values': False}\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_RUNS = []\n",
    "\n",
    "#Use itertools.product to get every combination of the four lists\n",
    "#The product will yield a tuple like (size, density, noise_level, negative_values)\n",
    "for size, density, noise_level, negative_values in product(\n",
    "    SIZE, DENSITY, NOISE_LEVEL, NEGATIVE_VALUES\n",
    "):\n",
    "    #Construct the dictionary for this specific run\n",
    "    run_params = {\n",
    "        'size': size,\n",
    "        'density': density,\n",
    "        'noise_level': noise_level,\n",
    "        'negative_values': negative_values,\n",
    "    }\n",
    "    EXPERIMENT_RUNS.append(run_params)\n",
    "\n",
    "print(f\"Total number of experiment runs generated: {len(EXPERIMENT_RUNS)}\")\n",
    "#Print the first few runs to verify the structure\n",
    "print(\"Example of the first 3 runs:\")\n",
    "for i in range(3):\n",
    "    print(EXPERIMENT_RUNS[i])\n",
    "\n",
    "#Resilience Check: Load Completed Runs\n",
    "\n",
    "def load_completed_keys(filename: str) -> set:\n",
    "    \"\"\"Reads the JSONL file and returns a set of completed parameter tuples.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return set()\n",
    "    \n",
    "    completed = set()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                p = data['parameters']\n",
    "                #Create a hashable key for parameter matching\n",
    "                key = (p['size'], p['density'], p['noise_level'], p['negative_values'])\n",
    "                completed.add(key)\n",
    "            except json.JSONDecodeError:\n",
    "                #Skip corrupted lines\n",
    "                continue\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total defined runs: 224\n",
      "Completed runs found: 218\n",
      "Remaining runs to execute: 6\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running APSP Experiments:   0%|          | 0/6 [1:06:52<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m matrix, coordinates = create_problem(**params)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m#Run the A* search and save the data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mrun_experiment_and_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem_matrix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstance_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#Update the progress bar status with current run details\u001b[39;00m\n\u001b[32m     33\u001b[39m pbar.set_postfix_str(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSize: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Neg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[33m'\u001b[39m\u001b[33mnegative_values\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mrun_experiment_and_save\u001b[39m\u001b[34m(problem_matrix, pos_map, instance_params, output_filename, approximation_threshold, num_hubs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_approximation:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m#APPROXIMATION MODE (for large N)\u001b[39;00m\n\u001b[32m     42\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     apsp_paths, total_nodes_explored = \u001b[43mhub_based_apsp_approximation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m        \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_hubs\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     end_time = time.time()\n\u001b[32m     49\u001b[39m     total_time = end_time - start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mhub_based_apsp_approximation\u001b[39m\u001b[34m(G, nodes, pos_map, p_map, k)\u001b[39m\n\u001b[32m     46\u001b[39m     dist_to_hubs[h][u] = \u001b[32m0.0\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m _, cost_uh, nodes_explored_uh = \u001b[43mreduced_cost_a_star_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m dist_to_hubs[h][u] = cost_uh\n\u001b[32m     51\u001b[39m total_nodes_explored += nodes_explored_uh\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mreduced_cost_a_star_search\u001b[39m\u001b[34m(graph, start_node, goal_node, pos_map, p_map)\u001b[39m\n\u001b[32m    104\u001b[39m g_score_prime[neighbor] = tentative_g_score_prime\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m#Calculate f' score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m h_prime_n = \u001b[43meuclidean_heuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m)\u001b[49m + p_map[v_idx]\n\u001b[32m    108\u001b[39m f_score_prime = tentative_g_score_prime + h_prime_n\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m#Push the neighbor onto the heap (O(log V) operation)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36meuclidean_heuristic\u001b[39m\u001b[34m(u, v, pos_map)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meuclidean_heuristic\u001b[39m(u: \u001b[38;5;28mint\u001b[39m, v: \u001b[38;5;28mint\u001b[39m, pos_map: np.ndarray) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# pos_map[u] gives the (x, y) coordinates for node u\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     distance = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m distance\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\PoliTo\\Anno 5\\Computational intelligence\\Labs\\Lab 3\\CI2025_lab3\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2792\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2790\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2791\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2792\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2793\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "OUTPUT_FILE = \"a_star_experiment_results.jsonl\"\n",
    "TOTAL_RUNS = len(EXPERIMENT_RUNS)\n",
    "\n",
    "completed_keys = load_completed_keys(OUTPUT_FILE)\n",
    "\n",
    "RESUMABLE_RUNS = []\n",
    "for params in EXPERIMENT_RUNS:\n",
    "    key = (params['size'], params['density'], params['noise_level'], params['negative_values'])\n",
    "    if key not in completed_keys:\n",
    "        RESUMABLE_RUNS.append(params)\n",
    "\n",
    "print(f\"Total defined runs: {TOTAL_RUNS}\")\n",
    "print(f\"Completed runs found: {len(completed_keys)}\")\n",
    "print(f\"Remaining runs to execute: {len(RESUMABLE_RUNS)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "pbar = tqdm(RESUMABLE_RUNS, desc=\"Running APSP Experiments\")\n",
    "\n",
    "for params in pbar:\n",
    "    try:\n",
    "        #Generate the problem instance\n",
    "        matrix, coordinates = create_problem(**params)\n",
    "        \n",
    "        #Run the A* search and save the data\n",
    "        run_experiment_and_save(\n",
    "            problem_matrix=matrix,\n",
    "            pos_map=coordinates,\n",
    "            instance_params=params,\n",
    "            output_filename=OUTPUT_FILE\n",
    "        )\n",
    "        \n",
    "        #Update the progress bar status with current run details\n",
    "        pbar.set_postfix_str(f\"Size: {params['size']}, Neg: {params['negative_values']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        #Crucial for resilience: Print error, but continue to the next run\n",
    "        print(f\"\\n‚ÄºÔ∏è ERROR processing run: {params}. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\\nüöÄ All scheduled experiments complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
