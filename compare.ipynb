{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "def load_experiment_data(filename: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Reads JSONL file and returns a list of experiment dictionaries,\n",
    "       handling string representations of infinity.\"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    #Custom function to convert string representations of infinity back to float\n",
    "    def parse_infinity(value):\n",
    "        if value == \"inf\":\n",
    "            return float('inf')\n",
    "        elif value == \"-inf\":\n",
    "            return -float('inf')\n",
    "        return value\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return data_list\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                #Load the JSON object\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                #Recursively parse through the dictionary to convert 'inf' strings\n",
    "                def deep_parse(obj):\n",
    "                    if isinstance(obj, dict):\n",
    "                        return {k: deep_parse(v) for k, v in obj.items()}\n",
    "                    elif isinstance(obj, list):\n",
    "                        return [deep_parse(elem) for elem in obj]\n",
    "                    elif isinstance(obj, str):\n",
    "                        return parse_infinity(obj)\n",
    "                    return obj\n",
    "                \n",
    "                data_list.append(deep_parse(data))\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON line: {e}\")\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_apsp_results(a_star_data: List[Dict[str, Any]], bf_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Compares A* (heuristic) results against Bellman-Ford (ground truth).\"\"\"\n",
    "    \n",
    "    bf_map = {}\n",
    "    \n",
    "    #Map Bellman-Ford data by parameter key for quick lookup\n",
    "    for bf_run in bf_data:\n",
    "        p = bf_run['parameters']\n",
    "        #Create a stable, hashable key (size, density, noise, neg_values)\n",
    "        key = (p['size'], p['density'], p['noise_level'], p['negative_values'])\n",
    "        bf_map[key] = bf_run\n",
    "\n",
    "    comparison_results = []\n",
    "\n",
    "    #Iterate through A* data and compare to the ground truth\n",
    "    for a_star_run in a_star_data:\n",
    "        p = a_star_run['parameters']\n",
    "        key = (p['size'], p['density'], p['noise_level'], p['negative_values'])\n",
    "        \n",
    "        if key not in bf_map:\n",
    "            #Skip if the corresponding ground truth run is missing\n",
    "            continue\n",
    "\n",
    "        bf_run = bf_map[key]\n",
    "        a_star_paths = a_star_run['a_star_paths']\n",
    "        bf_paths = bf_run['bellman_ford_paths']\n",
    "        \n",
    "        N = p['size']\n",
    "        total_pairs = N * (N - 1)\n",
    "        \n",
    "        #Accuracy Metrics\n",
    "        matched_costs = 0\n",
    "        total_valid_paths = 0\n",
    "        total_cost_diff = 0.0\n",
    "        \n",
    "        #Threshold for floating point comparison\n",
    "        EPSILON = 1e-6 \n",
    "        \n",
    "        #Check every pair (s->d and d->s)\n",
    "        for path_key in a_star_paths.keys():\n",
    "            a_cost = a_star_paths[path_key]['cost']\n",
    "            bf_cost = bf_paths.get(path_key, {'cost': float('nan')})['cost'] # Use .get() for safety\n",
    "\n",
    "            #Skip pairs that are not connected (cost == inf) or have negative cycles\n",
    "            if bf_cost == float('inf') or bf_cost == -float('inf'):\n",
    "                continue\n",
    "            \n",
    "            total_valid_paths += 1\n",
    "            \n",
    "            #Check if costs match within the epsilon tolerance\n",
    "            if abs(a_cost - bf_cost) < EPSILON:\n",
    "                matched_costs += 1\n",
    "            \n",
    "            #Calculate the approximation factor (approximation ratio)\n",
    "            if bf_cost > 0:\n",
    "                #Approximation factor: |Cost_approx - Cost_true| / Cost_true\n",
    "                total_cost_diff += abs(a_cost - bf_cost) / bf_cost\n",
    "        \n",
    "        #Compile Final Comparison Metrics\n",
    "        comparison = {\n",
    "            'parameters': p,\n",
    "            'N': N,\n",
    "            'approximation_used': a_star_run['metrics'].get('approximation_used', False), # Default for non-approximated runs\n",
    "            \n",
    "            #Performance\n",
    "            'time_A_star_sec': a_star_run['metrics']['total_time_seconds'],\n",
    "            'time_BF_sec': bf_run['metrics']['total_time_seconds'],\n",
    "            'speedup_factor': bf_run['metrics']['total_time_seconds'] / a_star_run['metrics']['total_time_seconds'],\n",
    "            'nodes_explored_A_star': a_star_run['metrics']['total_nodes_explored'],\n",
    "            \n",
    "            #Accuracy\n",
    "            'total_valid_paths': total_valid_paths,\n",
    "            'accuracy_rate': matched_costs / total_valid_paths if total_valid_paths > 0 else 0.0,\n",
    "            #Mean Approximation Ratio (MAR)\n",
    "            'mean_approx_ratio': total_cost_diff / total_valid_paths if total_valid_paths > 0 else float('nan'),\n",
    "        }\n",
    "        \n",
    "        comparison_results.append(comparison)\n",
    "\n",
    "    return comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIGURATION\n",
    "A_STAR_FILE = \"a_star_experiment_results.jsonl\"\n",
    "BF_FILE = \"bellman_ford_results.jsonl\"\n",
    "COMPARISON_OUTPUT_FILE = \"comparison_metrics.jsonl\"\n",
    "\n",
    "#Load Data\n",
    "print(\"Loading A* (Heuristic) Data...\")\n",
    "a_star_data = load_experiment_data(A_STAR_FILE)\n",
    "\n",
    "print(\"Loading Bellman-Ford (Ground Truth) Data...\")\n",
    "bf_data = load_experiment_data(BF_FILE)\n",
    "\n",
    "#Compare Results\n",
    "if a_star_data and bf_data:\n",
    "    print(f\"Comparing {len(a_star_data)} A* runs against {len(bf_data)} Bellman-Ford runs...\")\n",
    "    final_metrics = compare_apsp_results(a_star_data, bf_data)\n",
    "    \n",
    "    #Save Comparison Metrics\n",
    "    with open(COMPARISON_OUTPUT_FILE, 'w') as f:\n",
    "        for result in final_metrics:\n",
    "            json.dump(result, f)\n",
    "            f.write('\\n')\n",
    "            \n",
    "    print(f\"\\n✅ Comparison complete! Metrics saved to {COMPARISON_OUTPUT_FILE}\")\n",
    "    print(f\"Total metrics generated: {len(final_metrics)}\")\n",
    "else:\n",
    "    print(\"\\n❌ Cannot perform comparison: One or both data files are empty or missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb743c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "#Configuration\n",
    "COMPARISON_FILE = \"comparison_metrics.jsonl\"\n",
    "#Set Seaborn style for better visualization\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "#Data Loading and Preparation\n",
    "\n",
    "def load_comparison_data(filename: str = COMPARISON_FILE) -> pd.DataFrame:\n",
    "    \"\"\"Reads the comparison JSONL file, flattens the structure, and returns a DataFrame.\"\"\"\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Error: Comparison file not found at {filename}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.strip(): continue\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                #Flatten the structure\n",
    "                flat_data = {\n",
    "                    'N': data['N'],\n",
    "                    'approximation_used': data['approximation_used'],\n",
    "                    'time_A_star_sec': data['time_A_star_sec'],\n",
    "                    'time_BF_sec': data['time_BF_sec'],\n",
    "                    'speedup_factor': data['speedup_factor'],\n",
    "                    'nodes_explored_A_star': data['nodes_explored_A_star'],\n",
    "                    'accuracy_rate': data['accuracy_rate'],\n",
    "                    'mean_approx_ratio': data['mean_approx_ratio'],\n",
    "                    'Size': data['parameters']['size'],\n",
    "                    'Density': data['parameters']['density'],\n",
    "                    'Noise_Level': data['parameters']['noise_level'],\n",
    "                    'Negative': data['parameters']['negative_values'],\n",
    "                }\n",
    "                data_list.append(flat_data)\n",
    "            except Exception as e:\n",
    "                #Catch any loading or key errors and skip the line\n",
    "                print(f\"Skipping line due to error: {e}\") \n",
    "                \n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    #Ensure numerical types are correct\n",
    "    numeric_cols = ['Size', 'Density', 'time_A_star_sec', 'time_BF_sec', 'mean_approx_ratio', 'speedup_factor']\n",
    "    df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Load the data\n",
    "df_results = load_comparison_data()\n",
    "\n",
    "if df_results.empty:\n",
    "    print(\"Cannot proceed with plotting: DataFrame is empty.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Data loaded successfully. Total runs: {len(df_results)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "#Plotting Functions\n",
    "\n",
    "def plot_performance_vs_size(df):\n",
    "    \"\"\"\n",
    "    Plot 1: Log-Log plot of Time vs. Problem Size (N) comparing all methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Filter for non-negative runs for a clean comparison\n",
    "    df_plot = df[df['Negative'] == False]\n",
    "    \n",
    "    #Create a column that identifies the specific A* method used\n",
    "    df_plot['A_star_Method'] = df_plot.apply(\n",
    "        lambda row: 'Hub-Based Approx' if row['approximation_used'] else 'Exact A* (RC)', axis=1\n",
    "    )\n",
    "    \n",
    "    #Create long format for combined A* and BF plotting\n",
    "    df_long = pd.melt(df_plot, \n",
    "                      id_vars=['Size', 'A_star_Method'], \n",
    "                      value_vars=['time_BF_sec', 'time_A_star_sec'],\n",
    "                      var_name='Algorithm', \n",
    "                      value_name='Time (sec)')\n",
    "    \n",
    "    #Assign clear labels\n",
    "    df_long['Algorithm'] = df_long['Algorithm'].replace({\n",
    "        'time_BF_sec': 'Bellman-Ford (Ground Truth)',\n",
    "        'time_A_star_sec': 'A* Heuristic'\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.lineplot(\n",
    "        data=df_long,\n",
    "        x='Size',\n",
    "        y='Time (sec)',\n",
    "        hue='Algorithm',\n",
    "        style='A_star_Method', #Use the A* method to differentiate the lines\n",
    "        marker='o',\n",
    "        errorbar=('ci', 95)\n",
    "    )\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.title('Performance Scaling: Total Time vs. Problem Size (Log-Log Scale)')\n",
    "    plt.xlabel('Problem Size N (Number of Nodes) [Log Scale]')\n",
    "    plt.ylabel('Total Execution Time (Seconds) [Log Scale]')\n",
    "    plt.legend(title='Algorithm/Method', loc='upper left')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def plot_efficiency_vs_density(df):\n",
    "    \"\"\"\n",
    "    Plot 2: Efficiency (Nodes Explored Ratio) vs. Density.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Calculate the ratio: Nodes Explored / Total possible SSSP runs (N * N)\n",
    "    df['exploration_ratio'] = df['nodes_explored_A_star'] / (df['Size'] ** 2)\n",
    "    \n",
    "    #Group by density, calculating the mean ratio across all sizes\n",
    "    df_plot = df.groupby('Density', as_index=False)['exploration_ratio'].mean()\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.lineplot(x='Density', y='exploration_ratio', data=df_plot, marker='o', errorbar=('ci', 95))\n",
    "    \n",
    "    plt.title('A* Efficiency: Mean Exploration Ratio vs. Graph Density')\n",
    "    plt.xlabel('Graph Density')\n",
    "    plt.ylabel('Avg. Nodes Explored per SSSP Run (Normalized by N²)')\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", linewidth=0.5)\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def plot_accuracy_of_approximation(df):\n",
    "    \"\"\"\n",
    "    Plot 3: Accuracy (Mean Approximation Ratio - MAR) vs. Size for approximation runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Filter only the runs where approximation was used\n",
    "    df_approx = df[df['approximation_used'] == True].copy()\n",
    "    \n",
    "    if df_approx.empty:\n",
    "        print(\"Skipping Accuracy Plot: No approximation data found.\")\n",
    "        return\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    #Use seaborn boxplot to show the distribution of approximation errors\n",
    "    sns.boxplot(x='Size', y='mean_approx_ratio', hue='Density', data=df_approx, palette=\"viridis\")\n",
    "    \n",
    "    plt.title('Approximation Quality: Mean Approximation Ratio (MAR)')\n",
    "    plt.xlabel('Problem Size N (Approximation Runs)')\n",
    "    plt.ylabel('Mean Approximation Ratio (MAR)')\n",
    "    plt.legend(title='Density', loc='upper left')\n",
    "    plt.grid(True, which=\"major\", ls=\"--\", linewidth=0.5)\n",
    "    # Set ylim to focus on small errors (MAR should be near 0)\n",
    "    plt.ylim(0, df_approx['mean_approx_ratio'].quantile(0.95) * 1.1) \n",
    "    plt.show()\n",
    "\n",
    "#Run all Plots\n",
    "\n",
    "print(\"Generating Performance Plot (Time vs. Size)...\")\n",
    "plot_performance_vs_size(df_results)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Generating Efficiency Plot (Exploration vs. Density)...\")\n",
    "plot_efficiency_vs_density(df_results)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Generating Accuracy Plot (MAR vs. Size)...\")\n",
    "plot_accuracy_of_approximation(df_results)\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
